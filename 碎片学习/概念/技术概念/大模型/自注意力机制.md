### 自注意力机制的原理

简单来说，自注意力机制的工作原理是：模型在处理一个句子时，==会给句子中的每一个词都赋予一个“注意力分数”，这个分数代表它与句子中**所有其他词**（包括它自己）之间的关联度。==

例如，在处理句子“他把苹果放进了篮子，因为它太沉了。”时，模型需要理解“它”指的是“苹果”还是“篮子”。为了做到这一点，它会计算“它”与句子中所有其他词之间的注意力分数。

这个过程涉及三个关键向量：

- **Query（查询）**
    
- **Key（键）**
    
- **Value（值）**
    

模型会计算每个词的**查询向量**与所有其他词的**键向量**的点积（内积），得到注意力分数。然后用这些分数对**值向量**进行加权求和，得到该词新的表示。

### 为什么是指数级增长？

自注意力机制的计算复杂度是**O(n2)**，其中 n 是上下文的长度（即 Token 的数量）。

让我们来拆解一下这个二次方关系：

1. **注意力分数矩阵**：对于一个包含 n 个 Token 的上下文，模型需要计算一个 n×n 的注意力分数矩阵。这个矩阵中的每个元素，都代表了**一个 Token 和另一个 Token 之间的关联度**。
    
2. **计算量**：为了生成这个 n×n 的矩阵，模型需要进行 n×n 次向量点积运算。
    
3. **内存占用**：为了存储这个注意力分数矩阵，内存占用也会呈二次方增长。
    

举个例子：

- **上下文长度为 1000 个 Token**：计算量和内存占用与 10002=1,000,000 成正比。
    
- **上下文长度为 10000 个 Token**：计算量和内存占用将与 100002=100,000,000 成正比。
    

你会发现，当上下文长度增加 10 倍时，==计算量和内存占用会增加 **100 倍**。这就是为什么上下文大小的增长，会导致计算资源呈指数级增长。==

### 总结

因为大模型需要通过**自注意力机制**来理解长文本中任意两个词之间的关系，==而这个机制的计算复杂度是二次方的，所以上下文越大，所需的计算资源就会呈指数级增加。==

不过，研究人员正在积极探索新的技术，如稀疏注意力（Sparse Attention）和线性注意力（Linear Attention），来降低这种二次方复杂度的影响，从而在保证性能的同时，实现更长的上下文窗口。