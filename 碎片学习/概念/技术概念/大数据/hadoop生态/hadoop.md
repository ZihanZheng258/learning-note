**Hadoop** 是一个由 Apache 基金会开发的**开源框==架**，它的核心目标是**高效地存储和处理超大规模数据集**（从 GB 到 PB 甚至 EB 级别）==。它不是一个单一的工具，而是一个由多个核心组件和工具组成的生态系统，共同协作来解决大数据问题。

你可以把 Hadoop 理解为一个==“分布式计算和存储平台==”。在传统的计算模式中，一台服务器很难处理海量数据。Hadoop 的设计理念是==：**将数据分散存储在大量廉价的、普通的服务器上（形成一个集群），并同时将计算任务也分配到这些服务器上并行执行**==，从而实现对海量数据的高效处理和分析。

### 为什么需要 Hadoop？

在 Hadoop 出现之前，处理大数据面临以下主要挑战：

- **数据量巨大：** ==单台服务器的存储能力和计算能力是有限的，无法满足TB、PB级别数据的存储和处理需求。==

- **数据类型多样：** 传统数据库难以有效存储和处理结构化、半结构化和非结构化数据。

- **高成本：** 购买昂贵的大型服务器来处理大数据成本很高。

- **扩展性差：** 传统系统在数据量增长时，扩展能力有限。

- **容错性：** 单台服务器故障可能导致整个任务失败和数据丢失。

Hadoop 正是为了解决这些问题而诞生的，它通过**分布式**和**并行化**的方式，在**廉价的硬件**上提供了**高扩展性、高容错性**的大数据存储和处理能力。