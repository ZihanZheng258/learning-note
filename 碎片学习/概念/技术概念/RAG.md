RAG 是一种结合了**信息检索 (Retrieval)** 和 **文本生成 (Generation)** 的人工智能模型架构。它的主要目的是**增强大型语言模型 (LLM) 的能力**，让它们能够生成**更准确、更相关、更少幻觉**（即“胡说八道”）的回答，尤其是在处理**特定领域知识或最新信息**时。

### **为什么需要 RAG？大型语言模型的局限性**

要理解 RAG，首先要明白 LLM 的一些固有局限性：

1. **知识截止 (Knowledge Cutoff)**：LLM 的知识只停留在其训练数据的时间点。它们无法回答关于最新事件、产品或研究的问题。
2. **幻觉 (Hallucination)**：LLM 有时会“编造”事实，生成听起来合理但实际上是错误或不存在的信息。这是因为它们是概率模型，善于预测下一个词，而不是保证事实的准确性。
3. **缺乏特定领域知识 (Lack of Domain-Specific Knowledge)**：LLM 拥有广泛的通用知识，但对于某个行业、公司内部数据或高度专业化的领域，它们可能知之甚少或根本没有。
4. **难以溯源 (Lack of Attribution)**：LLM 生成的答案通常不提供信息来源，用户难以验证其真实性。

#### **阶段一：检索 (Retrieval)**

1. **用户提问 (Query)**：用户提出一个问题或提供一个查询。
2. **语义搜索/信息匹配**：RAG 系统会分析用户的查询，并使用某种**搜索机制**（通常是基于向量相似度的语义搜索）去一个预先准备好的**外部知识库（例如文档、数据库、网页集合等）**中查找**最相关的片段或文档**。
    - 这个知识库可以是任何结构化的或非结构化的数据源。
    - 在实际操作中，通常会将知识库中的文本切分成小块（chunks），并为每个小块生成一个**向量嵌入 (vector embedding)**。用户的查询也会被转换成一个向量。
    - 然后，系统会计算查询向量与所有文档块向量之间的相似度，找出最相似（即语义上最相关）的 Top-K 个文档块。
3. **获取上下文 (Context)**：==将检索到的最相关信息片段作为**上下文 (context)**。==

#### **阶段二：增强生成 (Augmented Generation)**

1. **LLM 输入**：==将用户的**原始查询**和**检索到的上下文信息**一起作为输入，发送给大型语言模型==。
    - 例如，输入可能看起来像这样：“根据以下信息：[检索到的相关文档片段]，请回答：[用户的问题]。”
2. **生成回答**：LLM 利用这个**增强的输入**来生成最终的答案。它不再是凭空“想象”答案，而是有了实实在在的“参考资料”。
3. **输出与溯源 (Optional Attribution)**：生成的答案通常会更准确、更具体。有些 RAG 系统还会提供**引用来源**，告诉用户答案中的信息来自知识库的哪个文档或哪个部分。